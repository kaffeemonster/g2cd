/*
 * this is all quite PD, nothing magic:
 * search introsort
 * search quicksort
 * search heapsort
 * search insertsort
 * glue it all together
 *
 * Thanks go to Ralph Unden for a paper about intro sort with
 * concrete java demonstration code, i'm better at comprehending
 * code than dry formal talk "given an set epsilon...".
 *
 * rewritten for uint32_t and C by me, so all errors are
 * mine - Jan
 */

#include "../config.h"
#include "other.h"
#include "my_bitops.h"
#include <stdbool.h>
#define MIN(a, b) (((a) < (b)) ? (a) : (b))
#define U32_BITS (sizeof(uint32_t) * BITS_PER_CHAR)

/*
 * introsort_u32 - sort an array of u32 with intro sort
 *
 * This sort tries to eliminate duplicates, it does not remove
 * every possible dublicate, but some.
 *
 * a[]: array of u32 to sort
 * n: number of u32 in a
 *
 * return value: number of "valid" output elements
 *
 * Intro sort is a variant of quick sort, which tries to fall back
 * to another sorting algorithm, when we are heading for the worst
 * case for quick sort (O(n^2)).
 * Rest is quite of the mill, except for the duplication removal.
 *
 * AGAIN: we remove duplicates, this is a deviation not reflected
 *        in the name and has to be changed when used as a normal
 *        generic sorting alg.
 */

/*
 * ~~~   Motivation   ~~~
 *
 * The reason we have this code is 3 fold:
 * - Mainly: not all libc qsort() are born equal, they sometimes miss
 *   the tuning and only some systems use introsort as qsort
 *   -> since this is in the scope of G2CD, we have to look at what
 *      we are sorting: hashes, generated by foreign input.
 *   -> On normal operation they are totaly random distributed, so
 *      algs which try to find preruns or such stuff don't pay off,
 *      and we don't care for stable
 *   -> but an attacker may craft an request to generate hashes in the
 *      worst-case order
 *   -> We really really want introsort to get the best of both worlds.
 *      (sure, hitting the heapsort fallback hurts, but...).
 *   We do not want to buy the cat in bag by calling just "any" qsort.
 * - While at it to remove the overhead of a generic lib sort. We
 *   really want to do simple number sorting, like textbook code does,
 *   so away with elemsize and compare callbacks, nice clean aligned
 *   access for the RISC archs.
 * - we expect low to modest n (search requests generating 1000'th
 *   of hashes are also large as packets (untransportable) and prop.
 *   malicious...), with a overhelming number in the low 1 to 20
 *   hashes. Sorting such a low number even n^2 is cheaper than
 *   hitting the unsorted case in the QHT lookups.
 *
 * Additionaly we try to remove duplicates, since we are traversing the
 * array and doing lots of compares anyway. This is not 100%, but at
 * least should find some to reduce the number of elements in the output
 * (and lookups in the (compressed) QHTs).
 */

/*
 * A note on all the "noinline":
 * This is not because this should never be inlined, but a whack over
 * GCC's head. GCC tends to be fantastic at inlining, but while doing
 * so, generates more and more large/complicated functions. That's
 * natural for inlining (kind of, yes, inlining can also remove all the
 * invariant handling, expose more stuff (CSE, Const. Prop., etc) and
 * make the resulting logic smaller...), but still not perfect/solved
 * after 300 bazillion man-years of fiddling whith more optimization-
 * passes and finding the right inline limits.
 *
 * But it may bite you in the ass since one part of GCC is then
 * overwhelmed by what all the other clever passes did before it:
 * GCC's frame-handling sucks balls.
 *
 * Those large and comlicated functions then have a large frame/need all
 * register (register allocator did the job it's supposed to do and put
 * everything in some reg) which then needs complicated handling on all
 * in-/egress-paths.
 *
 * GCC pulls out top function needs to the top outermost layer, often
 * accumulating needs because it does not properly overlays stackslots,
 * where it then sees: "Oh, i need all register, and 200 byte of scratch
 * ram, hey, lets alloc a gigantic stack and push all regs there before
 * we do *anything* else (on functions entry).".
 *
 * Instead of partitioning the function, or keeping some form of the original
 * function-partitioning, and doing it gradually where certain stack/regs
 * needs are only fullfilled later when really needed (when heavy/costly
 * part of inlined functions is really entered)).
 *
 * This is bad for small cases (suddenly large static cost in comparision
 * to done work). Also plays havok with recursion.
 *
 * Since GCC can also generate tail-calls, and some form of control
 * transfer (jumps, etc.) have to happen even in a large function
 * anyway and aren't that expensive anymore (and GCC generates non-ABI
 * calls on those static funcs), we get the advantage of lean small
 * cases while having nearly no down side.
 *
 * This is kind of a special problem of functions which are in between
 * "simple 3 line helper" (accessor-helper) and out-of-mind complicated
 * ("let's do a 3 Body Van-FooBar cross correlation in XML", 12 level deep
 * nested loop to follow), while having a "works good for all sizes of n,
 * where n is 1 to $INSERT_LARGE_NUMBER" subscript.
 *
 * Or to say it differently: If your complete function stack subject to
 * inlining handles small cases "ligther" in certain dimensions (and that
 * is important to you), then this property is damaged by GCCs inlining,
 * or more precise: lacking ability in some of it's passes and logic.
 */

/*
 * Another note, esp. on the above:
 * We assume to run on a modern "full-fledged" CPU. Instructions are
 * pipelined, we have a cache, yes, there is IPC, we have several
 * hundereds of MHz to throw at the problem.
 * So sometimes sticking to KISS, stuff that compiles to to nicer
 * ops, and spending less ops on bookkeeping even if you do more
 * calculation, surprisingly gets stuff done TM faster.
 */

/*
 * Be carefull when changing this value.
 * Due to quicksorts D&C aprouch, and our do-final-sort-at-the-
 * complete-end there is a "2 for the price of 1" hidden here:
 * When partioning went ideal, we half the array every time, so
 * those last SIZE_THRESHOLD "unsorted" values lay next to another
 * partition of SIZE_THRESHOLD "unsorted" values.
 * (This is _very_ rough, quicksort did it's thing on the way down
 * so they are not totaly unsorted, but the last steps before ending
 * the quicksort recursion on SIZE_THRESHOLD saw fewer pivoting).
 * This means more work for insertion sort, which is good for nearly
 * sorted arrays (why we use it as final sort), but quickly collapses
 * when presented with to much disorder.
 *
 * If you now scratch your head what i mean or want to send me flaming
 * emails about me talking nonsense ("The halves laying side by side are
 * parted by the pivot, what is he talking about?"):
 * This is some "how-do-i-put-it-into-words" talk after looking at lots
 * and lots of traces. Somehow insertion sort runs less smooth the
 * larger the combined length of the partions is.
 * (Propably becuase of the entropy removal every partioning does. This
 * value was always about finding the cutoff cost of more partioning and
 * gain of reduced entropy which would insertion sort a hard time)
 *
 * So take it "as is".
 */
#define SIZE_THRESHOLD 5

/*
 * size limit on the (binary) radix sort is mainly driven to avoid n*k
 * actually beeing n^2 or much worse
 */
#define RADIX_SIZE_THRESHOLD (U32_BITS*SIZE_THRESHOLD)

/*
 * Common helper
 */
static inline void exchange_u32(uint32_t *x, size_t a, size_t b)
{
	uint32_t t = x[a];
	x[a] = x[b];
	x[b] = t;
}

/*
 * heap sort as worst worst case fallback
 */
// TODO: this could be rewritten? (Floyd??)
/*
 * this way we get fewer compares, but "walk" the array 3 times...
 * so the question is how this hits the caches, esp. since
 * this should only be the fallback on deep recursion, so small
 * array partitions
 */
static void downheap_u32(uint32_t a[], size_t i, size_t n)
{
	uint32_t d = a[i - 1];
	size_t child;

	while((child = i * 2) <= n)
	{
		if(child < n && a[child - 1] < a[child])
			child++;
		if(!(d < a[child - 1]))
			break;
		a[i - 1] = a[child - 1];
		i = child;
	}
	a[i - 1] = d;
}

static noinline void heapsort_u32(uint32_t a[], size_t n)
{
	size_t i;
	for(i = n / 2; i >= 1; i--)
		downheap_u32(a, i, n);
	for(i = n; i > 1; i--) {
		exchange_u32(a, 0, i - 1);
		downheap_u32(a, 1, i - 1);
	}
}

/*
 * smoothsort as fallback
 *
 * smoothsort based on heapsort.
 * Like heapsort it achieves n+log(n) in the worst case, but due to
 * a custom heap based on Leonardo numbers it can achieve better than
 * n+log(n) performance, where heapsort is in every case n+log(n)
 */
static const uint32_t LP[] = {
	1, 1, 3, 5, 9, 15, 25, 41, 67, 109,
	177, 287, 465, 753, 1219, 1973, 3193, 5167, 8361, 13529, 21891,
	35421, 57313, 92735, 150049, 242785, 392835, 635621, 1028457,
	1664079, 2692537, 4356617, 7049155, 11405773, 18454929, 29860703,
	48315633, 78176337, 126491971, 204668309, 331160281, 535828591,
	866988873 /* the next number is > 31 bits */
};

static void sift_u32(uint32_t *head, unsigned int pshift)
{
	/* we do not use Floyd's improvements to the heapsort sift, because we
	 * are not doing what heapsort does - always moving nodes from near
	 * the bottom of the tree to the root. */
	uint32_t val = *head;
	while(pshift > 1)
	{
		uint32_t *rt = head - 1, *lf = head - 1 - LP[pshift - 2];

		if(val >= *lf && val >= *rt)
			break;

		if(*lf >= *rt) {
			*head = *lf;
			head = lf;
			pshift -= 1;
		} else {
			*head = *rt;
			head = rt;
			pshift -= 2;
		}
	}
	*head = val;
}

static void trinkle_u32(uint32_t *head, unsigned int p, unsigned int pshift, bool isTrusty)
{
	uint32_t val = *head;

	while (p != 1)
	{
		uint32_t *stepson = head - LP[pshift];

		if (*stepson <= val)
			break; /* current node is greater than head. sift. */

		/* no need to check this if we know the current node is trusty,
		 * because we just checked the head (which is val, in the first
		 * iteration) */
		if (!isTrusty && pshift > 1) {
			if (*(head - 1) >= *stepson ||
			    *(head - 1 - LP[pshift - 2]) >= *stepson)
				break;
		}

		*head = *stepson;
		head = stepson;
		int trail = __builtin_ctz(p & ~1);
		p >>= trail;
		pshift += trail;
		isTrusty = false;
	}

	if (!isTrusty) {
		*head = val;
		sift_u32(head, pshift);
	}
}

static noinline void smoothsort_u32(uint32_t A[], size_t n)
{
	size_t head = 0; /* the offset of the first element of the prefix into A */
	/*
	 * These variables need a little explaining. If our string of heaps
	 * is of length 38, then the heaps will be of size 25+9+3+1, which are
	 * Leonardo numbers 6, 4, 2, 1.
	 * Turning this into a binary number, we get b01010110 = 0x56. We represent
	 * this number as a pair of numbers by right-shifting all the zeros and
	 * storing the mantissa and exponent as "p" and "pshift".
	 * This is handy, because the exponent is the index into L[] giving the
	 * size of the rightmost heap, and because we can instantly find out if
	 * the rightmost two heaps are consecutive Leonardo numbers by checking
	 * (p&3)==3
	 */
	unsigned int p = 1; /* the bitmap of the current standard concatenation >> pshift */
	unsigned int pshift = 1;

	while(head < n)
	{
		if((p & 3) == 3)
		{
			/* Add 1 by merging the first two blocks into a larger one.
			 * The next Leonardo number is one bigger. */
			sift_u32(&A[head], pshift);
			p >>= 2;
			pshift += 2;
		}
		else
		{
			/* adding a new block of length 1 */
			if(LP[pshift - 1] >= n - head) /* this block is its final size. */
				trinkle_u32(&A[head], p, pshift, false);
			else /* this block will get merged. Just make it trusty. */
				sift_u32(&A[head], pshift);

			if (pshift == 1) {
				p <<= 1; /* LP[1] is being used, so we add use LP[0] */
				pshift--;
			} else {
				p <<= (pshift - 1); /* shift out to position 1, add LP[1] */
				pshift = 1;
			}
		}
		p |= 1;
		head++;
	}

	trinkle_u32(&A[head], p, pshift, false);

	while(pshift != 1 || p != 1)
	{
		if(pshift <= 1) {
			/* block of length 1. No fiddling needed */
			int trail = __builtin_ctz(p & ~1);
			p >>= trail;
			pshift += trail;
		}
		else
		{
			p <<= 2;
			p ^= 7;
			pshift -= 2;

			/* This block gets broken into three bits. The rightmost bit is a
			 * block of length 1. The left hand part is split into two, a block
			 * of length LP[pshift+1] and one of LP[pshift].  Both these two
			 * are appropriately heapified, but the root nodes are not
			 * necessarily in order. We therefore semitrinkle both of them */
			trinkle_u32(&A[head - LP[pshift] - 1], p >> 1, pshift + 1, true);
			trinkle_u32(&A[head - 1], p, pshift, true);
		}
		head--;
	}
}

/*
 * insertion sort which tries to removes duplicates.
 * The final sorting alg. to smooth out the roughly presorted
 * array. Suggested by Sedgewick
 */
static noinline size_t insertionsort_u32_c(uint32_t a[], size_t n)
{
	size_t i, j, k;
	uint32_t t;

	/* n can not be 0 here */
	i = 0; k = 0;
	goto start_loop;
	do
	{
		/*
		 * Since i don't see any super intelligent way to remove
		 * all duplicates, this simple check should remove most,
		 * since our input array is rougly presorted.
		 */
		if(a[i] == a[k - 1] ||
		   a[i] == t)
			continue;
start_loop:
		j = k;
		t = a[i];
		while(j && t < a[j - 1]) {
			a[j] = a[j - 1];
			j--;
		}
		a[j] = t;
		k++;
	} while(++i < n);
	return k;
}

/*
 * switcher
 */
static noinline void othersort_u32(uint32_t a[], size_t n)
{
	/* we use an int as bitmask p, so we can only sort so many elements */
	if(likely(n < LP[sizeof(int)*BITS_PER_CHAR]+1))
		smoothsort_u32(a, n);
	else
		heapsort_u32(a, n);
}

/*
 * intro sort
 */

/*
 * A good pivot-point for partitioning is crucial for qsort.
 * But finding it, besides of really searching for it, is an
 * art. Lot's of wise men came up with lot's of solutions...
 * But basically, we have (and want) to fudge it.
 * The precise criticallity of the pivot-point also depends
 * on the cost of your single operations. Since we are not the
 * libc api sort and specially typed for u32, our cost for
 * compares and swaps is much lowered. Still lowering compares
 * and swaps is good, since they are a strain on the cache-bw and
 * data dependent compares (and from that jumps) play havoc with
 * performance of modern pipelined chips.
 */
static uint32_t *medianof3_u32(uint32_t *lo, uint32_t *mid, uint32_t *hi)
{
#if 0
	uint32_t t;
	/* a sorting network which already does some sorting is slower */
	if(*mid > *hi)  {t = *mid; *mid = *hi;   *hi = t;}
	if( *lo > *hi)  {t = *lo;   *lo = *hi;   *hi = t;}
	if( *lo > *mid) {t = *lo;   *lo = *mid; *mid = t;}
	return mid;
#else
	return *mid < *lo ? (*hi < *mid ? mid : (*hi < *lo ? hi : lo)) :
	                    (*hi < *mid ? (*hi < *lo ? lo : hi) : mid);
#endif
}

static noinline uint32_t pseudo_median_u32(uint32_t a[], size_t n)
{
	uint32_t *mid = &a[n / 2 + 1]; /* take the middle element */
	/*
	 * only if the array is large enough bother searching
	 * for something else then the middle element.
	 * Even if pivot selection sucks in this case, more
	 * computation on the median does not pay off for so
	 * small arrays.
	 */
	if(n > 7)
	{
		uint32_t *lo = a, *hi = &a[n - 1];
		/*
		 * on real "big" arrays we can use Tukey's 'ninther'
		 * pseudomedian of nine
		 */
		if(unlikely(n > 40)) {
			size_t s = (n / 8);
			lo  = medianof3_u32(lo, lo+s, lo+2*s);
			mid = medianof3_u32(mid-s, mid, mid+s);
			hi  = medianof3_u32(hi-2*s, hi-s, hi);
		}
		mid = medianof3_u32(lo, mid, hi);
	}
	return *mid;
}

struct p_res
{
	size_t l,h;
};

/*
 * kind of 3-way partition the data:
 * - everything lower than the pivot left
 * - everything higher than the pivot right
 * - everything equal than pivot in a "dead zone" in the middle
 * this way runs (or simply lots of) the same number
 * get split out fast, and qsort does not choke on
 * them.
 */
#if 0
//TODO: buggy, +-1 error
static struct p_res partition_u32(uint32_t x[], size_t n, uint32_t v)
{
	size_t a,b,c,d,l,h,s;
	a = b = 0;
	c = d = n - 1;
	/* first put numers eq pivot at start and end while
	 * pivoting all other
	 */
	while(1)
	{
		for(; b < c && x[b] <= v; b++) {
			if(x[b] == v)
				exchange_u32(x, a++, b);
		}
		for(; b < c && x[c] >= v; c--) {
			if(x[c] == v)
				exchange_u32(x, d--, c);
		}
		if(b >= c)
			break;
		exchange_u32(x, b++, c--);
	}
	/* now put the eq numbers at start and end in the middle */
	s = MIN(a, b-a);
	for(l = 0, h = b - s; s; s--)
		exchange_u32(x, l++, h++);
	s = MIN(d-c, n - 1 - d);
	for(l = b, h = n - s; s; s--)
		exchange_u32(x, l++, h++);
	return (struct p_res){b - a, d - c};
}
#else
/* simpler partition, upside it's logically cheaper and this way faster */
static struct p_res partition_u32(uint32_t a[], size_t n, uint32_t x)
{
	size_t i = 0, j = n;

	while(1)
	{
		while(a[i] < x)
			i++;
		j--;
		while(x < a[j])
			j--;
		if(i >= j) {
			if(i > j)
				j++;
			return (struct p_res){i, n - j};
		}
		exchange_u32(a, i, j);
		i++;
	}
}
#endif

static noinline void introsort_loop_u32(uint32_t a[], size_t n, unsigned depth_limit);

static noinline void introsort_loop_u32_inner(uint32_t a[], size_t n, unsigned depth_limit)
{
	struct p_res p;
	size_t n_s, n_b;
	uint32_t *a_s, *a_b;

	prefetch(a);
	p = partition_u32(a, n, pseudo_median_u32(a, n));
	/*
	 * recurse into the smaller partition first (costs _real_
	 * stack), 'cause it should be easier to sort, while the
	 * bigger partition uses the tail recursion eliminated path.
	 * Suggested by Sedgewick
	 * This is, even more ops are used, faster...
	 */
	if(p.l < p.h) {
		a_s = a;             n_s = p.l;
		a_b = a + (n - p.h); n_b = p.h;
	} else {
		a_s = a + (n - p.h); n_s = p.h;
		a_b = a;             n_b = p.l;
	}
	introsort_loop_u32(a_s, n_s, depth_limit - 1);
	/*
	 * we could write a loop, but keep tail recursion for simplycity
	 * the compiler should be able to eliminate it
	 * this way we also do keep track of the depth limit
	 */
	introsort_loop_u32(a_b, n_b, depth_limit - 1);
}

static noinline void introsort_loop_u32(uint32_t a[], size_t n, unsigned depth_limit)
{
	/*
	 * only pre sort, where quick sort is most effective,
	 * final run will be something else
	 */
	if(n <= SIZE_THRESHOLD)
		return;

	/* throttle it? */
	if(unlikely(!depth_limit))
	{
		/*
		 * we left the "optimal" recursion depth, so we are prop. heading
		 * for the worst case, quickly switch to some other sorting
		 * alg. for this partition
		 */
		/*
		 * Backtracking because we hit the worst case sounds like a good
		 * idea, but our fallback sorts are only that: a fallback.
		 * Hitting them to often does not make things better, but if we
		 * are screwed, they still will save our ass.
		 */
		othersort_u32(a, n);
		return;
	}

	introsort_loop_u32_inner(a, n, depth_limit);
}

/*
 * This is a kind of comb sort.
 * It's tailored for our case of being between an qsort not run
 * to the last element and the insertion sort that follows.
 * It is not a real full comb sort, do not copy and use it as one.
 */
static noinline void combsort(uint32_t a[], size_t n)
{
	size_t gap = SIZE_THRESHOLD;

	while(1)
	{
		size_t i, j = n;
		gap = (gap * 99u) / 128u; /* ~ gap / 1.3 */
		if(!(gap > 1))
			break;

		for(i = 0; i + gap < j; i++)
		{
			if(a[i] > a[i+gap])
				exchange_u32(a, i, i+gap);
		}
	}
}

size_t introsort_u32(uint32_t a[], size_t n)
{
	prefetch(a);
	if(n < 2)
		return n;
	/*
	 * Theoretically radix sort is kind of made for sorting
	 * numbers. There is only one problem:
	 * It's cost amortisation is very bad for small n.
	 * It also has problems with non uniformly distributed keys.
	 *
	 * So it could be beneficial to start off oportunistically
	 * a radix sort first and use something else if we detect
	 * one of the problems above.
	 * Problem is: more complexity, and in our setup qsort
	 * beats it anyway due to more complex logic.
	 *
	 * A Simple 1 Bit, in place, MSB one looks like a qsort
	 * only with different pivot selection, more complex n at
	 * a time radix sorts suck, too complex, to much time spent
	 * with bookkeeping when todays MHz could just get jiggy.
	 *
	 * I drag-raced a 4bit, in place, MSB against this code,
	 * for 2^26 entrys the difference was 1 second.
	 * For the much lower n we expect? Not a chance.
	 * Yes, this ratio could be changed more in Radix-sort favor,
	 * but at a cost. No in-place? Allocs for the workarea.
	 * More bits at a time? The space for bookkeeping gets really large.
	 * LSB? Would lose the ability to use insertionsort as last sort
	 * when buckets get really small and radix-sort gets inefficient.
	 *
	 * All this fancy Big-O() talk drops all the other terms
	 * to fast IMHO and seldomly looks at the costs of ops.
	 * That's not a secret, but too often pushed into the footnote
	 * in the persuit of pure CS and "mine's bigger than yours".
	 */
	if(n > SIZE_THRESHOLD) /* pull out to eleminate limit calc on small arrays */
		introsort_loop_u32(a, n, ((flsst(n)*21u)/16u) - 1 /* ~ floor(ld(n)*1.3) */);
	/*
	 * Do a final run to really sort things and remove dublicates.
	 * Note that we simply stoped sorting when we reached a certain
	 * threshhold and then go over the complete array again at the
	 * complete end (like Sedgewick proposed, but that where the
	 * days of fear of call overhead. Optimizing compilers? Pipelined
	 * CPU? Whats that witchcraft you are talking about?)
	 * This is bad for CPU-caches (instead of calling the final
	 * sort right there and now when we hit the threshhold when the
	 * elements are cache warm), which is more and more important
	 * with the increasing CPU to Memory speed gap.
	 * (Mauser already tried in his orig. introsort, not much effect,
	 * except sometimes the negative, but that where the days of
	 * SPARC-II beeing hip...)
	 * But at least for us it has to stay that way, it is a tribute
	 * to dublicate removal (more important to us), and it's not
	 * like the setup of every small sort comes for free...
	 */
	/*
	 * First run some rounds of dumbed down combsort.
	 * Only do it for small arrays, they saw fewer pivoting (or none
	 * at all). Larger arrays asymptotically move against limit,
	 * and last sort is a nuance, while this dumbed down combsort
	 * with it's ogre "hugh, must sort" approuch whould void all
	 * the savings done.
	 * This is limited to gaps between the qsort stop condition and
	 * larger than 1. With our limit of 5 should make 2 runs with
	 * gap 3 and 2.
	 * Small arrays which where not pivoted at least one time by qsort
	 * also get some first love here, so insertionsort does not face
	 * such a hard time.
	 * Nice thing about combsort is the low overhead (still not low
	 * enough to call on every last partioning...).
	 */
	if(n <= 3*SIZE_THRESHOLD)
		combsort(a, n);
	/*
	 * then do the insertionssort with minimal unorderedness, while
	 * removing dublicates. (removes dublicates best when unorderedness
	 * is not to great)
	 */
	return insertionsort_u32_c(a, n);
}

/*@unused@*/
static char const rcsid_is[] GCC_ATTR_USED_VAR = "$Id: $";
/* EOF */
